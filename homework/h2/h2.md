# HW2

## Ex.1

1. Please check `createcsv.awk` and `createcsv.sh` for details.
2. Run `hdfs getconf -confKey dfs.blocksize` and result on my computer is `134217728`. It seems that `fs.block.size` in the hint no longer exists in Hadoop 3.2.2.

## Ex.2

1. **Filecrush** is a tool to turn many small files into fewer larger ones. It can also change from text to sequence and other compression options in one pass. We can use it to turn many small files generated during the MapReduce process into a whole file that is easier to handle. For example, in this case, we can combine all the small `csv` file into one big crushed file.

2. The option `-Dfs.block.size=128000000` seems not working on Hadoop 3.2.2. Other options' meaning are listed as follows.
   - `--input-format text`: Fully qualified class name of the input format for the data in a directory. Can use the "text" and "sequence" shortcuts for org.apache.hadoop.mapred.TextInputFormat and org.apache.hadoop.mapred.SequenceFileInputFormat, respectively. Here we set it to be `text`.
   - `--output-format sequence`: Same function as `--input-format text` except this is for the format of the input. Here we set it to be `sequence`.
   - `--clone`: Use clone mode. Useful for external Hive tables. In clone mode, the small files are replaced with the larger files. The small files are moved to a subdirectory of the output dir argument. The subdirectory is same as the original directory rooted at output dir. For example, assume the input dir argument and output dir argument are /user/example/input and /user/example/output, respectively. If a file was originally /user/example/input/my-dir/smallfile, then after the clone, the original file would be located in /user/example/output/user/example/input/my-dir/smallfile.
   - `--compress gzip`: Fully qualified class name of the compression codec to use when writing data. It is permissible to use "none" and "gzip" to indicate no compression and org.apache.hadoop.io.compress.GzipCodec, respectively. Here we use `gzip` to compress it.
   
3. I made two attempts, respectively with or without clone mode.

   In the first attempt without clone mode, I run 

   ```bash
   hadoop jar target/filecrush-2.2.2-SNAPSHOT.jar com.m6d.filecrush.crush.Crush --input-format text --output-format sequence --compress gzip input output `date +%Y%m%d%H%M%S`
   ```

   The input looks like this:

   <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210603184902068.png" alt="image-20210603184902068" style="zoom: 80%;" />

   The output looks like this:

   <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210603184935577.png" alt="image-20210603184935577" style="zoom:80%;" />

   We can see 10 small files are successfully compressed into one big file.

   In the second attempt with clone mode, I run

   ```bash
   hadoop jar target/filecrush-2.2.2-SNAPSHOT.jar com.m6d.filecrush.crush.Crush --input-format text --output-format sequence --clone --compress gzip input output `date +%Y%m%d%H%M%S`
   ```

   The input looks like this:

   <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210603185504592.png" alt="image-20210603185504592" style="zoom:80%;" />

   The output looks like this:

   <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210603185535600.png" alt="image-20210603185535600" style="zoom:80%;" />

   With the `--clone` option on, the input files are moved to `output/user/hadoop/input` and the small files in the original input directory is replaced by the large crushed file.

   Both attempts seem successful.

## Ex.3

1. The `--groupBy` is  regular expression that causes S3DistCp to concatenate files that match the expression. For example, you could use this option to combine all of the log files written in one hour into a single file. We can run `s3-dist-cp --src hdfs:///usr/hadoop/input --dest hdfs:///usr/local/output --groupBy='*.csv'` to concatenate all .csv files in the src directory.

## Ex.4

1. **Snappy** is a fast data compression and decompression library written in C++ by Google. It does not aim for maximum compression, compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. Therefore, it is a very suitable for jobs with large amount of data, such as MapReduce and Bigtable.

   Check the `avro` directory for all the codes. First, we run `compactSmallFiles.serialize` to compact small files into a large compact file, `compact.avro`. Secondly, we run `extractSmallFiles.deserialize` to extract small files from the large file generated in the first step.

